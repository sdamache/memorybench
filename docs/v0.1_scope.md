# MemoryBench v0.1 Scope

**Version**: 0.1 (Hackathon Demo)
**Status**: Complete
**Last Updated**: 2025-12-23

## Why This Project Exists

Memory providers for AI applications are fragmented. Developers face:

- **Fragmentation**: Each memory provider has its own API, setup process, and evaluation methodology
- **Slow Setup**: Getting a benchmark running against multiple providers takes significant effort
- **No Standard Comparison**: Hard to objectively compare memory providers without a unified testing framework

MemoryBench solves this by providing a **unified benchmarking platform** where developers can:
1. Add new memory **providers** with a standard interface
2. Add new **benchmarks** to test against
3. Run all combinations via a single CLI

---

## v0.1 Deliverables

The following items must be complete for v0.1 to be considered "done":

| Deliverable | Description | Completion Criterion |
|-------------|-------------|---------------------|
| Unified Runner CLI | Single entry point to run benchmarks | `bun run index.ts --help` shows usage |
| Provider Interface | Standard interface for memory providers | `providers/_template/index.ts` exists with `TemplateType` export |
| Benchmark Interface | Standard interface for benchmarks | `benchmarks/index.ts` exports `BenchmarkRegistry` type |
| Results Writer | Structured output from benchmark runs | Console output shows benchmark results per provider |
| Results Explorer | View and compare results | Results displayed in terminal with provider comparison |
| Documentation | Scope and usage documentation | `docs/v0.1_scope.md` exists (this file) |

### Deliverables Checklist

- [x] Unified runner CLI accepts `--benchmarks` and `--providers` flags
- [x] At least 2 providers implemented and registered (5 implemented: LocalBaseline, AQRAG, ContextualRetrieval, Supermemory, Mem0)
- [x] At least 1 benchmark implemented and registered (3 implemented: RAG-template-benchmark, LongMemEval, LoCoMo)
- [x] CLI outputs benchmark results to console
- [x] README links to scope documentation
- [x] Demo script runs without errors
- [x] Results explorer for viewing benchmark results
- [x] Checkpoint/resume for interrupted runs
- [x] Multi-backend LLM judge support

---

## v0.1 Targets

### Providers (Target: 2) - Exceeded: 5 Implemented

| Provider | Type | Status | Location |
|----------|------|--------|----------|
| LocalBaseline | hybrid | Implemented | `providers/LocalBaseline/` |
| ContextualRetrieval | intelligent_memory | Implemented | `providers/ContextualRetrieval/` |
| AQRAG | intelligent_memory | Implemented | `providers/AQRAG/` |
| Supermemory | intelligent_memory | Implemented | `providers/supermemory/` |
| Mem0 | intelligent_memory | Implemented | `providers/mem0/` |

### Provider Storage Note

Providers that use Postgres store their tables in separate database schemas so they can coexist in a single database without conflicts. For example, AQRAG uses `aqrag.*` and ContextualRetrieval uses `contextual_retrieval.*`. Use `bun scripts/verify-db.ts` to confirm the schemas and tables exist.

### Benchmarks (Target: 1 Primary) - Exceeded: 3 Implemented

| Benchmark | Type | Status | Location |
|-----------|------|--------|----------|
| RAG-template-benchmark | Template | Implemented | `benchmarks/RAG-template-benchmark/` |
| LongMemEval | Research | Implemented | `benchmarks/LongMemEval/` |
| LoCoMo | Research | Implemented | `benchmarks/LoCoMo/` |

---

## v0.1 Output Artifacts

When a benchmark run completes, the following outputs are produced in `runs/{run_id}/`:

| Artifact | Format | Description | Status |
|----------|--------|-------------|--------|
| Console Output | Text | Real-time progress and results | Implemented |
| run_manifest.json | JSON | Run metadata (timestamp, providers, benchmarks, git commit, environment) | Implemented |
| results.jsonl | JSONL | Per-item benchmark results (appended incrementally, crash-safe) | Implemented |
| metrics_summary.json | JSON | Aggregated metrics across all items | Implemented |
| checkpoint.json | JSON | Checkpoint state for resume capability | Implemented |

See [docs/output-format.md](output-format.md) for complete schema documentation.

---

## Demo Script

### Prerequisites

- [Bun](https://bun.sh) installed (v1.0+)
- Repository cloned locally

### Step 1: Install Dependencies

```bash
bun install
```

### Step 2: Run Baseline Benchmark (No API Keys Required)

```bash
bun run index.ts eval --providers LocalBaseline --benchmarks RAG-template-benchmark
```

Expected output: Benchmark runs against LocalBaseline provider (in-memory, BM25), showing progress and results.

### Step 3: Run Comparison Benchmark (Multiple Providers)

```bash
bun run index.ts eval --providers LocalBaseline AQRAG --benchmarks RAG-template-benchmark
```

Expected output: Benchmark runs against both providers, allowing comparison of results.

### Step 4: Explore Results

```bash
# List available runs
ls runs/

# Launch interactive explorer
bun run index.ts explore --run <run_id> --port 3000
```

### CLI Reference

```
Usage:
  bun run index.ts eval --providers <name1> [name2...] --benchmarks <name1> [name2...]
  bun run index.ts list providers [--json]
  bun run index.ts list benchmarks [--json]
  bun run index.ts explore --run <run_id> [--port <port>]

Eval Options:
  --providers, -p     Providers to test (LocalBaseline, AQRAG, etc.)
  --benchmarks, -b    Benchmark types to run (RAG-template-benchmark, LongMemEval, LoCoMo)
  --concurrency       Number of parallel cases to run (default: 1)
  --resume            Resume an interrupted run by ID
```

---

## Non-Goals (Out of Scope for v0.1)

The following features are explicitly **not** part of v0.1:

| Feature | Reason | Target Version |
|---------|--------|----------------|
| Cloud Deployment | Local development only | v0.2+ |
| Performance Optimizations | Correctness over speed for v0.1 | v0.2+ |
| CI/CD Pipelines | Manual testing sufficient for demo | v0.2+ |

**Completed beyond original scope:**
- Results Explorer (interactive web UI for viewing results)
- Structured File Output (run_manifest.json, results.jsonl, metrics_summary.json)
- Multiple Benchmarks (3 total: RAG-template-benchmark, LongMemEval, LoCoMo)
- Result Persistence (runs saved to disk with checkpoint/resume)
- Multiple LLM Judge Backends (6 backends supported)

---

## Definition of Done

v0.1 is **complete**. All criteria met:

1. **All deliverables checked**: Every item in the deliverables checklist is marked complete
2. **Demo script works**: Commands in demo script execute without errors
3. **Documentation exists**: This scope document is complete and linked from README
4. **Five providers functional**: LocalBaseline, ContextualRetrieval, AQRAG, Supermemory, Mem0 all run successfully
5. **Three benchmarks functional**: RAG-template-benchmark, LongMemEval, LoCoMo execute and produce results
6. **Results persistence**: Runs saved to disk with checkpoint/resume capability
7. **Results explorer**: Interactive web UI for browsing results

---

## Maintaining This Document

If CLI flags or commands change:
1. Update the Demo Script section
2. Update the CLI Reference
3. Verify demo commands still work
