/**
 * Shared Types for Evaluation Protocols
 *
 * This module defines the interfaces for evaluation protocols used by
 * data-driven benchmarks. Each protocol implements a specific evaluation
 * strategy (LLM-as-judge, exact match, deletion check, etc.).
 *
 * @module src/evaluation/types
 * @see specs/006-benchmark-interface/data-driven-benchmark-design.md
 */

import type { RetrievalItem } from "../../types/core";

/**
 * Result from an evaluation protocol
 */
export interface EvaluationResult {
	/** Overall correctness score (0-1) */
	correctness: number;
	/** Whether answer is grounded in retrieved evidence (0-1) */
	faithfulness: number;
	/** Explanation of the evaluation reasoning */
	reasoning: string;
	/** Type-specific score based on question category (optional) */
	typeSpecificScore?: number;
	/** Additional protocol-specific metrics */
	additionalMetrics?: Record<string, number>;
}

/**
 * Context provided to evaluation protocols
 */
export interface EvaluationContext {
	/** The question/query being answered */
	question: string;
	/** The expected/ground truth answer */
	expectedAnswer: string;
	/** The answer generated by the memory system */
	generatedAnswer: string;
	/** Context retrieved from memory */
	retrievedContext: string[];
	/** Retrieved items with scores (for retrieval metrics) */
	retrievalResults?: RetrievalItem[];
	/** Question type for type-specific evaluation (optional) */
	questionType?: string;
	/** Additional metadata from the benchmark case */
	metadata?: Record<string, unknown>;
}

/**
 * Configuration for LLM-as-Judge evaluation
 */
export interface LLMJudgeConfig {
	/** Model to use for evaluation */
	model?: string;
	/** Google Cloud region for Vertex AI */
	region?: string;
	/** Google Cloud project ID */
	projectId?: string;
	/** Field in case input containing question type */
	typeField?: string;
	/** Type-specific evaluation instructions */
	typeInstructions?: Record<string, string>;
	/** Path to JSON file with type instructions (relative to benchmark dir) */
	typeInstructionsFile?: string;
}

/**
 * Configuration for exact match evaluation
 */
export interface ExactMatchConfig {
	/** Whether comparison is case-sensitive (default: false) */
	caseSensitive?: boolean;
	/** Whether to normalize whitespace (default: true) */
	normalizeWhitespace?: boolean;
	/** Whether to trim leading/trailing whitespace (default: true) */
	trim?: boolean;
}

/**
 * Configuration for deletion check evaluation
 */
export interface DeletionCheckConfig {
	/** Field containing queries for verification */
	verificationQueryField: string;
	/** Field containing content that should NOT appear */
	deletedContentField: string;
	/** Whether to use fuzzy matching (default: false) */
	fuzzyMatch?: boolean;
	/** Threshold for fuzzy match similarity (default: 0.8) */
	fuzzyThreshold?: number;
}

/**
 * Evaluation protocol interface
 *
 * All evaluation protocols must implement this interface.
 * Protocols are created via factory functions that accept configuration.
 */
export interface EvaluationProtocol {
	/** Protocol name for logging and identification */
	readonly name: string;

	/**
	 * Evaluate an answer given the context
	 *
	 * @param context - All information needed for evaluation
	 * @returns Evaluation result with scores and reasoning
	 */
	evaluate(context: EvaluationContext): Promise<EvaluationResult>;
}

/**
 * Factory function type for creating evaluation protocols
 */
export type EvaluationProtocolFactory<TConfig> = (
	config: TConfig,
) => EvaluationProtocol;
