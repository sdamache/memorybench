[
  {
    "result_id": "result_001",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_geo_001",
    "status": "passed",
    "started_at": "2025-01-15T10:00:01.000Z",
    "completed_at": "2025-01-15T10:00:03.250Z",
    "duration_ms": 2250,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.89,
      "retrieval_latency_ms": 145,
      "generation_latency_ms": 1850,
      "precision_at_k": 0.85,
      "recall": 0.90,
      "f1_score": 0.87,
      "exact_match": 1,
      "semantic_similarity": 0.92
    },
    "query": "What is the capital of France?",
    "expected": "Paris",
    "actual": "Paris",
    "retrieved_context": ["France is a country in Western Europe.", "Paris is the capital and largest city of France.", "The Eiffel Tower is located in Paris."]
  },
  {
    "result_id": "result_002",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_bio_001",
    "status": "passed",
    "started_at": "2025-01-15T10:00:04.000Z",
    "completed_at": "2025-01-15T10:00:06.100Z",
    "duration_ms": 2100,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.91,
      "retrieval_latency_ms": 132,
      "generation_latency_ms": 1720,
      "precision_at_k": 0.88,
      "recall": 0.85,
      "f1_score": 0.86,
      "exact_match": 1,
      "semantic_similarity": 0.94
    },
    "query": "What process do plants use to convert sunlight to energy?",
    "expected": "Photosynthesis",
    "actual": "Photosynthesis",
    "retrieved_context": ["Plants contain chlorophyll in their leaves.", "Photosynthesis is the process by which plants convert light energy into chemical energy."]
  },
  {
    "result_id": "result_003",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_crypto_001",
    "status": "failed",
    "started_at": "2025-01-15T10:00:07.000Z",
    "completed_at": "2025-01-15T10:00:09.500Z",
    "duration_ms": 2500,
    "metrics": {
      "retrieval_count": 2,
      "avg_relevance_score": 0.65,
      "retrieval_latency_ms": 178,
      "generation_latency_ms": 2100,
      "precision_at_k": 0.50,
      "recall": 0.40,
      "f1_score": 0.44,
      "exact_match": 0,
      "semantic_similarity": 0.58
    },
    "error": {
      "code": "ANSWER_MISMATCH",
      "message": "Expected answer did not match actual response",
      "context": {"expected_keywords": ["AES", "256-bit"], "found_keywords": ["encryption"]}
    },
    "query": "What is the standard symmetric encryption algorithm used in TLS 1.3?",
    "expected": "AES-256-GCM",
    "actual": "AES encryption is commonly used",
    "retrieved_context": ["TLS provides secure communication over networks.", "Encryption protects data in transit."]
  },
  {
    "result_id": "result_004",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_history_001",
    "status": "passed",
    "started_at": "2025-01-15T10:00:10.000Z",
    "completed_at": "2025-01-15T10:00:12.300Z",
    "duration_ms": 2300,
    "metrics": {
      "retrieval_count": 5,
      "avg_relevance_score": 0.88,
      "retrieval_latency_ms": 155,
      "generation_latency_ms": 1900,
      "precision_at_k": 0.82,
      "recall": 0.88,
      "f1_score": 0.85,
      "exact_match": 1,
      "semantic_similarity": 0.91
    },
    "query": "Who was the first President of the United States?",
    "expected": "George Washington",
    "actual": "George Washington",
    "retrieved_context": ["George Washington served as the first President.", "The Constitution was ratified in 1788."]
  },
  {
    "result_id": "result_005",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_science_001",
    "status": "errored",
    "started_at": "2025-01-15T10:00:13.000Z",
    "completed_at": "2025-01-15T10:00:15.100Z",
    "duration_ms": 2100,
    "metrics": {
      "retrieval_count": 0,
      "avg_relevance_score": 0,
      "retrieval_latency_ms": 2100
    },
    "error": {
      "code": "PROVIDER_TIMEOUT",
      "message": "Provider did not respond within the configured timeout",
      "stack": "TimeoutError: Request timed out after 2000ms\n    at ..."
    },
    "query": "What is the speed of light in a vacuum?",
    "expected": "299,792,458 meters per second",
    "actual": null
  },
  {
    "result_id": "result_006",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_geo_001",
    "status": "passed",
    "started_at": "2025-01-15T10:01:00.000Z",
    "completed_at": "2025-01-15T10:01:02.100Z",
    "duration_ms": 2100,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.93,
      "retrieval_latency_ms": 98,
      "generation_latency_ms": 1750,
      "precision_at_k": 0.92,
      "recall": 0.95,
      "f1_score": 0.93,
      "exact_match": 1,
      "semantic_similarity": 0.96
    },
    "query": "What is the capital of France?",
    "expected": "Paris",
    "actual": "Paris",
    "retrieved_context": ["Paris is the capital city of France.", "France is located in Western Europe.", "Paris has a population of over 2 million."]
  },
  {
    "result_id": "result_007",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_bio_001",
    "status": "passed",
    "started_at": "2025-01-15T10:01:03.000Z",
    "completed_at": "2025-01-15T10:01:04.900Z",
    "duration_ms": 1900,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.94,
      "retrieval_latency_ms": 87,
      "generation_latency_ms": 1600,
      "precision_at_k": 0.95,
      "recall": 0.92,
      "f1_score": 0.93,
      "exact_match": 1,
      "semantic_similarity": 0.97
    },
    "query": "What process do plants use to convert sunlight to energy?",
    "expected": "Photosynthesis",
    "actual": "Photosynthesis",
    "retrieved_context": ["Photosynthesis converts light energy to chemical energy in plants.", "Chloroplasts are the organelles where photosynthesis occurs."]
  },
  {
    "result_id": "result_008",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_crypto_001",
    "status": "passed",
    "started_at": "2025-01-15T10:01:05.000Z",
    "completed_at": "2025-01-15T10:01:07.200Z",
    "duration_ms": 2200,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.88,
      "retrieval_latency_ms": 112,
      "generation_latency_ms": 1850,
      "precision_at_k": 0.85,
      "recall": 0.88,
      "f1_score": 0.86,
      "exact_match": 1,
      "semantic_similarity": 0.89
    },
    "query": "What is the standard symmetric encryption algorithm used in TLS 1.3?",
    "expected": "AES-256-GCM",
    "actual": "AES-256-GCM",
    "retrieved_context": ["TLS 1.3 uses AES-256-GCM for symmetric encryption.", "AES is a block cipher standard."]
  },
  {
    "result_id": "result_009",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_history_001",
    "status": "passed",
    "started_at": "2025-01-15T10:01:08.000Z",
    "completed_at": "2025-01-15T10:01:10.100Z",
    "duration_ms": 2100,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.91,
      "retrieval_latency_ms": 95,
      "generation_latency_ms": 1780,
      "precision_at_k": 0.90,
      "recall": 0.92,
      "f1_score": 0.91,
      "exact_match": 1,
      "semantic_similarity": 0.94
    },
    "query": "Who was the first President of the United States?",
    "expected": "George Washington",
    "actual": "George Washington",
    "retrieved_context": ["George Washington was the first President of the United States, serving from 1789 to 1797."]
  },
  {
    "result_id": "result_010",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_science_001",
    "status": "passed",
    "started_at": "2025-01-15T10:01:11.000Z",
    "completed_at": "2025-01-15T10:01:13.300Z",
    "duration_ms": 2300,
    "metrics": {
      "retrieval_count": 2,
      "avg_relevance_score": 0.95,
      "retrieval_latency_ms": 105,
      "generation_latency_ms": 1950,
      "precision_at_k": 0.92,
      "recall": 0.88,
      "f1_score": 0.90,
      "exact_match": 1,
      "semantic_similarity": 0.93
    },
    "query": "What is the speed of light in a vacuum?",
    "expected": "299,792,458 meters per second",
    "actual": "299,792,458 meters per second",
    "retrieved_context": ["The speed of light in a vacuum is exactly 299,792,458 meters per second.", "Light is the fastest thing in the universe."]
  },
  {
    "result_id": "result_011",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "LoCoMo",
    "item_id": "locomo_001",
    "status": "passed",
    "started_at": "2025-01-15T10:02:00.000Z",
    "completed_at": "2025-01-15T10:02:03.500Z",
    "duration_ms": 3500,
    "metrics": {
      "retrieval_count": 6,
      "avg_relevance_score": 0.82,
      "retrieval_latency_ms": 210,
      "generation_latency_ms": 2800,
      "precision_at_k": 0.78,
      "recall": 0.85,
      "f1_score": 0.81,
      "semantic_similarity": 0.84
    },
    "query": "What was discussed in our last meeting about the project timeline?",
    "expected": "The project deadline was extended by two weeks to March 15th",
    "actual": "The project deadline was extended by two weeks",
    "retrieved_context": ["Meeting notes from Jan 10: Project timeline discussed", "Deadline extension approved to March 15th"]
  },
  {
    "result_id": "result_012",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "LoCoMo",
    "item_id": "locomo_002",
    "status": "failed",
    "started_at": "2025-01-15T10:02:04.000Z",
    "completed_at": "2025-01-15T10:02:07.200Z",
    "duration_ms": 3200,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.68,
      "retrieval_latency_ms": 195,
      "generation_latency_ms": 2650,
      "precision_at_k": 0.55,
      "recall": 0.62,
      "f1_score": 0.58,
      "semantic_similarity": 0.65
    },
    "error": {
      "code": "CONTEXT_RETRIEVAL_FAILED",
      "message": "Retrieved context did not contain sufficient information to answer the question accurately"
    },
    "query": "What were Alice's preferences for the team lunch location?",
    "expected": "Alice preferred Italian restaurants, specifically mentioned Olive Garden",
    "actual": "Alice mentioned she wanted to go out for lunch",
    "retrieved_context": ["Team discussion about lunch plans", "Several options were suggested"]
  },
  {
    "result_id": "result_013",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "LoCoMo",
    "item_id": "locomo_003",
    "status": "passed",
    "started_at": "2025-01-15T10:02:08.000Z",
    "completed_at": "2025-01-15T10:02:11.100Z",
    "duration_ms": 3100,
    "metrics": {
      "retrieval_count": 5,
      "avg_relevance_score": 0.86,
      "retrieval_latency_ms": 188,
      "generation_latency_ms": 2600,
      "precision_at_k": 0.82,
      "recall": 0.88,
      "f1_score": 0.85,
      "semantic_similarity": 0.87
    },
    "query": "When is Bob's birthday and what gift did we decide on?",
    "expected": "Bob's birthday is January 25th, we decided on a gift card",
    "actual": "Bob's birthday is January 25th, the team decided on a gift card",
    "retrieved_context": ["Bob mentioned his birthday is Jan 25", "Team agreed on gift card as present"]
  },
  {
    "result_id": "result_014",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "LoCoMo",
    "item_id": "locomo_004",
    "status": "passed",
    "started_at": "2025-01-15T10:02:12.000Z",
    "completed_at": "2025-01-15T10:02:15.400Z",
    "duration_ms": 3400,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.84,
      "retrieval_latency_ms": 175,
      "generation_latency_ms": 2900,
      "precision_at_k": 0.80,
      "recall": 0.85,
      "f1_score": 0.82,
      "semantic_similarity": 0.86
    },
    "query": "What were the action items from the standup on Monday?",
    "expected": "Review PR #234, Update documentation, Schedule design review",
    "actual": "Action items: Review PR #234, Update documentation, Schedule design review meeting",
    "retrieved_context": ["Monday standup notes", "Action items assigned to team members"]
  },
  {
    "result_id": "result_015",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "LoCoMo",
    "item_id": "locomo_001",
    "status": "passed",
    "started_at": "2025-01-15T10:03:00.000Z",
    "completed_at": "2025-01-15T10:03:02.800Z",
    "duration_ms": 2800,
    "metrics": {
      "retrieval_count": 5,
      "avg_relevance_score": 0.91,
      "retrieval_latency_ms": 125,
      "generation_latency_ms": 2400,
      "precision_at_k": 0.88,
      "recall": 0.92,
      "f1_score": 0.90,
      "semantic_similarity": 0.93
    },
    "query": "What was discussed in our last meeting about the project timeline?",
    "expected": "The project deadline was extended by two weeks to March 15th",
    "actual": "The project deadline was extended by two weeks to March 15th",
    "retrieved_context": ["Meeting from January 10: Timeline discussion", "Project deadline moved to March 15", "Two week extension approved"]
  },
  {
    "result_id": "result_016",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "LoCoMo",
    "item_id": "locomo_002",
    "status": "passed",
    "started_at": "2025-01-15T10:03:03.000Z",
    "completed_at": "2025-01-15T10:03:05.600Z",
    "duration_ms": 2600,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.89,
      "retrieval_latency_ms": 118,
      "generation_latency_ms": 2200,
      "precision_at_k": 0.85,
      "recall": 0.90,
      "f1_score": 0.87,
      "semantic_similarity": 0.91
    },
    "query": "What were Alice's preferences for the team lunch location?",
    "expected": "Alice preferred Italian restaurants, specifically mentioned Olive Garden",
    "actual": "Alice preferred Italian restaurants and mentioned Olive Garden as her top choice",
    "retrieved_context": ["Alice: I'd love Italian food", "Alice suggested Olive Garden", "Team lunch planning discussion"]
  },
  {
    "result_id": "result_017",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "LoCoMo",
    "item_id": "locomo_003",
    "status": "passed",
    "started_at": "2025-01-15T10:03:06.000Z",
    "completed_at": "2025-01-15T10:03:08.500Z",
    "duration_ms": 2500,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.93,
      "retrieval_latency_ms": 108,
      "generation_latency_ms": 2150,
      "precision_at_k": 0.90,
      "recall": 0.95,
      "f1_score": 0.92,
      "semantic_similarity": 0.95
    },
    "query": "When is Bob's birthday and what gift did we decide on?",
    "expected": "Bob's birthday is January 25th, we decided on a gift card",
    "actual": "Bob's birthday is January 25th, and the team decided on a gift card",
    "retrieved_context": ["Bob's birthday: January 25th", "Gift decision: $50 gift card"]
  },
  {
    "result_id": "result_018",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "LoCoMo",
    "item_id": "locomo_004",
    "status": "failed",
    "started_at": "2025-01-15T10:03:09.000Z",
    "completed_at": "2025-01-15T10:03:11.800Z",
    "duration_ms": 2800,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.72,
      "retrieval_latency_ms": 135,
      "generation_latency_ms": 2400,
      "precision_at_k": 0.65,
      "recall": 0.70,
      "f1_score": 0.67,
      "semantic_similarity": 0.71
    },
    "error": {
      "code": "PARTIAL_ANSWER",
      "message": "Response only partially matched expected answer"
    },
    "query": "What were the action items from the standup on Monday?",
    "expected": "Review PR #234, Update documentation, Schedule design review",
    "actual": "Review PR #234 and update documentation",
    "retrieved_context": ["Monday standup: Review PR", "Documentation updates needed"]
  },
  {
    "result_id": "result_019",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_math_001",
    "status": "passed",
    "started_at": "2025-01-15T10:04:00.000Z",
    "completed_at": "2025-01-15T10:04:02.200Z",
    "duration_ms": 2200,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.87,
      "retrieval_latency_ms": 142,
      "generation_latency_ms": 1800,
      "precision_at_k": 0.85,
      "recall": 0.88,
      "f1_score": 0.86,
      "exact_match": 1,
      "semantic_similarity": 0.90
    },
    "query": "What is the Pythagorean theorem?",
    "expected": "a^2 + b^2 = c^2",
    "actual": "a^2 + b^2 = c^2",
    "retrieved_context": ["The Pythagorean theorem states that in a right triangle, a^2 + b^2 = c^2"]
  },
  {
    "result_id": "result_020",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_math_001",
    "status": "passed",
    "started_at": "2025-01-15T10:04:03.000Z",
    "completed_at": "2025-01-15T10:04:05.100Z",
    "duration_ms": 2100,
    "metrics": {
      "retrieval_count": 2,
      "avg_relevance_score": 0.95,
      "retrieval_latency_ms": 92,
      "generation_latency_ms": 1780,
      "precision_at_k": 0.95,
      "recall": 0.92,
      "f1_score": 0.93,
      "exact_match": 1,
      "semantic_similarity": 0.97
    },
    "query": "What is the Pythagorean theorem?",
    "expected": "a^2 + b^2 = c^2",
    "actual": "a^2 + b^2 = c^2",
    "retrieved_context": ["Pythagorean theorem: a^2 + b^2 = c^2 for right triangles"]
  },
  {
    "result_id": "result_021",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_lit_001",
    "status": "failed",
    "started_at": "2025-01-15T10:04:06.000Z",
    "completed_at": "2025-01-15T10:04:08.500Z",
    "duration_ms": 2500,
    "metrics": {
      "retrieval_count": 2,
      "avg_relevance_score": 0.62,
      "retrieval_latency_ms": 165,
      "generation_latency_ms": 2100,
      "precision_at_k": 0.50,
      "recall": 0.55,
      "f1_score": 0.52,
      "exact_match": 0,
      "semantic_similarity": 0.58
    },
    "error": {
      "code": "WRONG_AUTHOR",
      "message": "Incorrect author attribution"
    },
    "query": "Who wrote '1984'?",
    "expected": "George Orwell",
    "actual": "Aldous Huxley",
    "retrieved_context": ["1984 is a dystopian novel", "Brave New World is another famous dystopian novel by Aldous Huxley"]
  },
  {
    "result_id": "result_022",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_lit_001",
    "status": "passed",
    "started_at": "2025-01-15T10:04:09.000Z",
    "completed_at": "2025-01-15T10:04:11.200Z",
    "duration_ms": 2200,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.92,
      "retrieval_latency_ms": 98,
      "generation_latency_ms": 1870,
      "precision_at_k": 0.90,
      "recall": 0.93,
      "f1_score": 0.91,
      "exact_match": 1,
      "semantic_similarity": 0.94
    },
    "query": "Who wrote '1984'?",
    "expected": "George Orwell",
    "actual": "George Orwell",
    "retrieved_context": ["1984 was written by George Orwell", "George Orwell's real name was Eric Arthur Blair"]
  },
  {
    "result_id": "result_023",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_tech_001",
    "status": "passed",
    "started_at": "2025-01-15T10:04:12.000Z",
    "completed_at": "2025-01-15T10:04:14.100Z",
    "duration_ms": 2100,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.85,
      "retrieval_latency_ms": 148,
      "generation_latency_ms": 1720,
      "precision_at_k": 0.82,
      "recall": 0.85,
      "f1_score": 0.83,
      "exact_match": 1,
      "semantic_similarity": 0.88
    },
    "query": "What programming language was TypeScript built on?",
    "expected": "JavaScript",
    "actual": "JavaScript",
    "retrieved_context": ["TypeScript is a superset of JavaScript", "TypeScript compiles to JavaScript"]
  },
  {
    "result_id": "result_024",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_tech_001",
    "status": "passed",
    "started_at": "2025-01-15T10:04:15.000Z",
    "completed_at": "2025-01-15T10:04:16.900Z",
    "duration_ms": 1900,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.94,
      "retrieval_latency_ms": 88,
      "generation_latency_ms": 1600,
      "precision_at_k": 0.92,
      "recall": 0.95,
      "f1_score": 0.93,
      "exact_match": 1,
      "semantic_similarity": 0.96
    },
    "query": "What programming language was TypeScript built on?",
    "expected": "JavaScript",
    "actual": "JavaScript",
    "retrieved_context": ["TypeScript extends JavaScript with static types"]
  },
  {
    "result_id": "result_025",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_chem_001",
    "status": "passed",
    "started_at": "2025-01-15T10:04:17.000Z",
    "completed_at": "2025-01-15T10:04:19.300Z",
    "duration_ms": 2300,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.88,
      "retrieval_latency_ms": 155,
      "generation_latency_ms": 1900,
      "precision_at_k": 0.85,
      "recall": 0.88,
      "f1_score": 0.86,
      "exact_match": 1,
      "semantic_similarity": 0.91
    },
    "query": "What is the chemical symbol for gold?",
    "expected": "Au",
    "actual": "Au",
    "retrieved_context": ["Gold has the chemical symbol Au from the Latin 'aurum'"]
  },
  {
    "result_id": "result_026",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "RAG-template-benchmark",
    "item_id": "rag_chem_001",
    "status": "passed",
    "started_at": "2025-01-15T10:04:20.000Z",
    "completed_at": "2025-01-15T10:04:22.000Z",
    "duration_ms": 2000,
    "metrics": {
      "retrieval_count": 2,
      "avg_relevance_score": 0.96,
      "retrieval_latency_ms": 82,
      "generation_latency_ms": 1700,
      "precision_at_k": 0.95,
      "recall": 0.94,
      "f1_score": 0.94,
      "exact_match": 1,
      "semantic_similarity": 0.98
    },
    "query": "What is the chemical symbol for gold?",
    "expected": "Au",
    "actual": "Au",
    "retrieved_context": ["Gold: Symbol Au, Atomic Number 79"]
  },
  {
    "result_id": "result_027",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "LoCoMo",
    "item_id": "locomo_005",
    "status": "errored",
    "started_at": "2025-01-15T10:04:23.000Z",
    "completed_at": "2025-01-15T10:04:25.500Z",
    "duration_ms": 2500,
    "metrics": {
      "retrieval_count": 0,
      "avg_relevance_score": 0,
      "retrieval_latency_ms": 2500
    },
    "error": {
      "code": "CONNECTION_ERROR",
      "message": "Failed to connect to vector database",
      "stack": "ConnectionError: ECONNREFUSED 127.0.0.1:5432"
    },
    "query": "What was the budget discussed for Q2?",
    "expected": "$50,000 for marketing, $30,000 for development",
    "actual": null
  },
  {
    "result_id": "result_028",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "LoCoMo",
    "item_id": "locomo_005",
    "status": "passed",
    "started_at": "2025-01-15T10:04:26.000Z",
    "completed_at": "2025-01-15T10:04:28.400Z",
    "duration_ms": 2400,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.88,
      "retrieval_latency_ms": 115,
      "generation_latency_ms": 2050,
      "precision_at_k": 0.85,
      "recall": 0.88,
      "f1_score": 0.86,
      "semantic_similarity": 0.89
    },
    "query": "What was the budget discussed for Q2?",
    "expected": "$50,000 for marketing, $30,000 for development",
    "actual": "Q2 budget: $50,000 allocated for marketing and $30,000 for development",
    "retrieved_context": ["Q2 Budget Meeting notes", "Marketing: $50k, Development: $30k"]
  },
  {
    "result_id": "result_029",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "LoCoMo",
    "item_id": "locomo_006",
    "status": "passed",
    "started_at": "2025-01-15T10:04:29.000Z",
    "completed_at": "2025-01-15T10:04:31.800Z",
    "duration_ms": 2800,
    "metrics": {
      "retrieval_count": 5,
      "avg_relevance_score": 0.83,
      "retrieval_latency_ms": 168,
      "generation_latency_ms": 2400,
      "precision_at_k": 0.80,
      "recall": 0.82,
      "f1_score": 0.81,
      "semantic_similarity": 0.85
    },
    "query": "What was Carol's feedback on the new UI design?",
    "expected": "Carol liked the color scheme but suggested larger fonts for accessibility",
    "actual": "Carol appreciated the colors and recommended larger fonts",
    "retrieved_context": ["Carol's UI feedback", "Color scheme approved", "Font size concerns raised"]
  },
  {
    "result_id": "result_030",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "LoCoMo",
    "item_id": "locomo_006",
    "status": "passed",
    "started_at": "2025-01-15T10:04:32.000Z",
    "completed_at": "2025-01-15T10:04:34.300Z",
    "duration_ms": 2300,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.90,
      "retrieval_latency_ms": 102,
      "generation_latency_ms": 1980,
      "precision_at_k": 0.88,
      "recall": 0.90,
      "f1_score": 0.89,
      "semantic_similarity": 0.92
    },
    "query": "What was Carol's feedback on the new UI design?",
    "expected": "Carol liked the color scheme but suggested larger fonts for accessibility",
    "actual": "Carol liked the color scheme but suggested larger fonts for better accessibility",
    "retrieved_context": ["Carol: Love the new colors!", "Carol: Can we make fonts bigger for accessibility?"]
  },
  {
    "result_id": "result_031",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "LoCoMo",
    "item_id": "locomo_007",
    "status": "failed",
    "started_at": "2025-01-15T10:04:35.000Z",
    "completed_at": "2025-01-15T10:04:37.600Z",
    "duration_ms": 2600,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.58,
      "retrieval_latency_ms": 185,
      "generation_latency_ms": 2200,
      "precision_at_k": 0.45,
      "recall": 0.52,
      "f1_score": 0.48,
      "semantic_similarity": 0.55
    },
    "error": {
      "code": "INCOMPLETE_RETRIEVAL",
      "message": "Failed to retrieve relevant context for the query"
    },
    "query": "What time zones do our remote team members work in?",
    "expected": "PST, EST, and GMT time zones",
    "actual": "Team members work remotely",
    "retrieved_context": ["Remote work policy", "Team collaboration tools"]
  },
  {
    "result_id": "result_032",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "LoCoMo",
    "item_id": "locomo_007",
    "status": "passed",
    "started_at": "2025-01-15T10:04:38.000Z",
    "completed_at": "2025-01-15T10:04:40.200Z",
    "duration_ms": 2200,
    "metrics": {
      "retrieval_count": 3,
      "avg_relevance_score": 0.91,
      "retrieval_latency_ms": 95,
      "generation_latency_ms": 1880,
      "precision_at_k": 0.90,
      "recall": 0.92,
      "f1_score": 0.91,
      "semantic_similarity": 0.93
    },
    "query": "What time zones do our remote team members work in?",
    "expected": "PST, EST, and GMT time zones",
    "actual": "Our remote team works in PST, EST, and GMT time zones",
    "retrieved_context": ["Team timezone distribution: PST, EST, GMT", "Remote work coordination"]
  },
  {
    "result_id": "result_033",
    "run_id": "run_2025_01_15_001",
    "provider": "AQRAG",
    "benchmark": "LoCoMo",
    "item_id": "locomo_008",
    "status": "passed",
    "started_at": "2025-01-15T10:04:41.000Z",
    "completed_at": "2025-01-15T10:04:43.500Z",
    "duration_ms": 2500,
    "metrics": {
      "retrieval_count": 4,
      "avg_relevance_score": 0.85,
      "retrieval_latency_ms": 162,
      "generation_latency_ms": 2100,
      "precision_at_k": 0.82,
      "recall": 0.85,
      "f1_score": 0.83,
      "semantic_similarity": 0.87
    },
    "query": "What was the outcome of the client meeting last Friday?",
    "expected": "Client approved the proposal and signed the contract",
    "actual": "The client approved our proposal and signed the contract",
    "retrieved_context": ["Friday client meeting", "Proposal approved", "Contract signed"]
  },
  {
    "result_id": "result_034",
    "run_id": "run_2025_01_15_001",
    "provider": "ContextualRetrieval",
    "benchmark": "LoCoMo",
    "item_id": "locomo_008",
    "status": "failed",
    "started_at": "2025-01-15T10:04:44.000Z",
    "completed_at": "2025-01-15T10:04:46.400Z",
    "duration_ms": 2400,
    "metrics": {
      "retrieval_count": 2,
      "avg_relevance_score": 0.68,
      "retrieval_latency_ms": 125,
      "generation_latency_ms": 2050,
      "precision_at_k": 0.60,
      "recall": 0.65,
      "f1_score": 0.62,
      "semantic_similarity": 0.66
    },
    "error": {
      "code": "MISSING_DETAILS",
      "message": "Response missing key details from expected answer"
    },
    "query": "What was the outcome of the client meeting last Friday?",
    "expected": "Client approved the proposal and signed the contract",
    "actual": "The client meeting went well",
    "retrieved_context": ["Client meeting scheduled for Friday", "Meeting notes pending"]
  }
]
